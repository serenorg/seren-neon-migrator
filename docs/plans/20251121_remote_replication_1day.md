# Remote Replication Implementation - 1 Day Plan

## Problem Statement

Users run replication on laptops with unreliable WiFi. Network interruptions cause multi-hour jobs to fail. Solution: Run replication on AWS EC2 with stable network, while user's CLI just submits jobs and polls for status.

## Solution Architecture (Simplified MVP)

```
User Laptop (CLI)  ‚Üí  AWS API Gateway  ‚Üí  Lambda  ‚Üí  EC2 Worker
                           ‚Üì
                       DynamoDB (job state)
```

**Key Design Decisions:**
- **Remote by default**: No `--local` flag for MVP, just remote execution
- **Simplified auth**: Hardcoded API endpoint, no auth for MVP (add later)
- **Single instance type**: c5.2xlarge for all jobs
- **Polling only**: No webhooks or real-time updates
- **Direct credential pass**: No KMS encryption for MVP (add security later)

## Day Structure (8 Hours)

Each task is 30-45 minutes with frequent commits. Follow TDD: write test first, implement, verify, commit.

---

## Hour 1: Foundation and Data Models

### Task 1.1: Create remote module structure (30 min)

**What:** Set up the Rust module structure for remote execution.

**Why:** Need a clean separation between local and remote execution logic.

**Files to create:**
1. `src/remote/mod.rs`
2. `src/remote/models.rs`
3. `src/remote/client.rs`

**Steps:**

1. **Create src/remote/mod.rs**

```rust
// ABOUTME: Remote execution module for running replication jobs on AWS
// ABOUTME: Handles job submission, status polling, and log retrieval

pub mod models;
pub mod client;

pub use models::{JobSpec, JobStatus, JobResponse};
pub use client::RemoteClient;
```

2. **Create src/remote/models.rs**

```rust
// ABOUTME: Data structures for remote job specifications and responses
// ABOUTME: These are serialized to JSON for API communication

use serde::{Deserialize, Serialize};
use std::collections::HashMap;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JobSpec {
    pub version: String,
    pub command: String, // "init" or "sync"
    pub source_url: String,
    pub target_url: String,
    pub filter: Option<FilterSpec>,
    pub options: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FilterSpec {
    pub include_databases: Option<Vec<String>>,
    pub exclude_tables: Option<Vec<String>>,
}

#[derive(Debug, Clone, Deserialize)]
pub struct JobResponse {
    pub job_id: String,
    pub status: String,
}

#[derive(Debug, Clone, Deserialize)]
pub struct JobStatus {
    pub job_id: String,
    pub status: String, // "provisioning", "running", "completed", "failed"
    pub created_at: Option<String>,
    pub started_at: Option<String>,
    pub completed_at: Option<String>,
    pub progress: Option<ProgressInfo>,
    pub error: Option<String>,
}

#[derive(Debug, Clone, Deserialize)]
pub struct ProgressInfo {
    pub current_database: Option<String>,
    pub databases_completed: usize,
    pub databases_total: usize,
}
```

3. **Update src/lib.rs to export remote module**

```rust
// Add this line near other module declarations
pub mod remote;
```

4. **Add dependencies to Cargo.toml**

```toml
[dependencies]
# Add these to existing dependencies
reqwest = { version = "0.11", features = ["json"] }
serde_json = "1.0"
```

**Testing:**

```bash
# Verify it compiles
cargo build

# Should see no errors, just compiling new modules
```

**Commit:**

```bash
git add src/remote/
git add src/lib.rs
git add Cargo.toml
git commit -m "Add remote execution module structure

- Create remote module with models and client
- Define JobSpec, JobStatus, JobResponse structures
- Add reqwest dependency for HTTP client"
```

---

### Task 1.2: Create remote HTTP client skeleton (30 min)

**What:** Build the HTTP client that talks to AWS API Gateway.

**Why:** CLI needs to submit jobs and poll for status.

**Files to modify:**
1. `src/remote/client.rs`

**Steps:**

1. **Create src/remote/client.rs**

```rust
// ABOUTME: HTTP client for communicating with remote execution API
// ABOUTME: Handles job submission, status polling, and error handling

use anyhow::{Context, Result};
use reqwest::Client;
use std::time::Duration;

use super::models::{JobSpec, JobResponse, JobStatus};

pub struct RemoteClient {
    client: Client,
    api_base_url: String,
}

impl RemoteClient {
    pub fn new(api_base_url: String) -> Result<Self> {
        let client = Client::builder()
            .timeout(Duration::from_secs(30))
            .build()
            .context("Failed to create HTTP client")?;

        Ok(Self {
            client,
            api_base_url,
        })
    }

    pub async fn submit_job(&self, spec: &JobSpec) -> Result<JobResponse> {
        let url = format!("{}/jobs", self.api_base_url);

        let response = self
            .client
            .post(&url)
            .json(spec)
            .send()
            .await
            .context("Failed to submit job")?;

        if !response.status().is_success() {
            let status = response.status();
            let body = response.text().await.unwrap_or_default();
            anyhow::bail!("Job submission failed with status {}: {}", status, body);
        }

        let job_response: JobResponse = response
            .json()
            .await
            .context("Failed to parse job response")?;

        Ok(job_response)
    }

    pub async fn get_job_status(&self, job_id: &str) -> Result<JobStatus> {
        let url = format!("{}/jobs/{}", self.api_base_url, job_id);

        let response = self
            .client
            .get(&url)
            .send()
            .await
            .context("Failed to get job status")?;

        if !response.status().is_success() {
            let status = response.status();
            let body = response.text().await.unwrap_or_default();
            anyhow::bail!("Failed to get job status {}: {}", status, body);
        }

        let job_status: JobStatus = response
            .json()
            .await
            .context("Failed to parse job status")?;

        Ok(job_status)
    }

    pub async fn poll_until_complete(
        &self,
        job_id: &str,
        callback: impl Fn(&JobStatus),
    ) -> Result<JobStatus> {
        loop {
            let status = self.get_job_status(job_id).await?;
            callback(&status);

            match status.status.as_str() {
                "completed" | "failed" => return Ok(status),
                _ => {
                    tokio::time::sleep(Duration::from_secs(5)).await;
                }
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_client_creation() {
        let client = RemoteClient::new("https://api.example.com".to_string());
        assert!(client.is_ok());
    }
}
```

**Testing:**

```bash
# Run tests
cargo test remote::client::tests

# Should see 1 test passing
```

**Commit:**

```bash
git add src/remote/client.rs
git commit -m "Add RemoteClient for API communication

- Implement submit_job() for job submission
- Implement get_job_status() for status polling
- Implement poll_until_complete() with callback
- Add basic unit test"
```

---

## Hour 2: CLI Integration

### Task 2.1: Add --remote flag to init command (30 min)

**What:** Add a `--remote` flag to the `init` command that switches between local and remote execution.

**Why:** Users need a way to opt into remote execution. For MVP, it's opt-in (later we'll make it default).

**Files to modify:**
1. `src/commands/init.rs`

**Steps:**

1. **Add --remote flag to init command arguments**

Find the `#[derive(Parser)]` struct for init command (around line 20-50). Add:

```rust
#[derive(Parser)]
pub struct InitArgs {
    // ... existing fields ...

    /// Execute replication remotely on AWS infrastructure
    #[arg(long)]
    pub remote: bool,

    /// API endpoint for remote execution (defaults to Seren's API)
    #[arg(long, default_value = "https://api.seren.cloud/replication")]
    pub remote_api: String,
}
```

2. **Add remote execution branch in init command**

Find the main `pub async fn init(args: InitArgs)` function. At the very top, add:

```rust
pub async fn init(args: InitArgs) -> Result<()> {
    // Remote execution path
    if args.remote {
        return init_remote(args).await;
    }

    // Local execution path (existing code continues below)
    // ... rest of existing init code ...
}
```

3. **Implement init_remote function**

Add this new function in `src/commands/init.rs`:

```rust
async fn init_remote(args: InitArgs) -> Result<()> {
    use crate::remote::{RemoteClient, JobSpec, FilterSpec};
    use std::collections::HashMap;

    println!("üåê Remote execution mode enabled");
    println!("API endpoint: {}", args.remote_api);

    // Build job specification
    let mut filter_spec = FilterSpec {
        include_databases: args.include_databases.clone(),
        exclude_tables: args.exclude_tables.clone(),
    };

    // If filter is empty, don't send it
    let filter = if filter_spec.include_databases.is_none()
        && filter_spec.exclude_tables.is_none()
    {
        None
    } else {
        Some(filter_spec)
    };

    let mut options = HashMap::new();
    options.insert(
        "drop_existing".to_string(),
        serde_json::Value::Bool(args.drop_existing),
    );
    options.insert(
        "yes".to_string(),
        serde_json::Value::Bool(args.yes),
    );

    let job_spec = JobSpec {
        version: "1".to_string(),
        command: "init".to_string(),
        source_url: args.source.clone(),
        target_url: args.target.clone(),
        filter,
        options,
    };

    // Submit job
    let client = RemoteClient::new(args.remote_api.clone())?;
    println!("Submitting replication job...");

    let response = client.submit_job(&job_spec).await?;
    println!("‚úì Job submitted");
    println!("Job ID: {}", response.job_id);
    println!("\nPolling for status...");

    // Poll until complete
    let final_status = client
        .poll_until_complete(&response.job_id, |status| {
            match status.status.as_str() {
                "provisioning" => println!("Status: provisioning EC2 instance..."),
                "running" => {
                    if let Some(ref progress) = status.progress {
                        println!(
                            "Status: running ({}/{}): {}",
                            progress.databases_completed,
                            progress.databases_total,
                            progress.current_database.as_deref().unwrap_or("unknown")
                        );
                    } else {
                        println!("Status: running...");
                    }
                }
                _ => {}
            }
        })
        .await?;

    // Display result
    match final_status.status.as_str() {
        "completed" => {
            println!("\n‚úì Replication completed successfully");
            Ok(())
        }
        "failed" => {
            let error_msg = final_status.error.unwrap_or_else(|| "Unknown error".to_string());
            println!("\n‚úó Replication failed: {}", error_msg);
            anyhow::bail!("Replication failed");
        }
        _ => {
            anyhow::bail!("Unexpected final status: {}", final_status.status);
        }
    }
}
```

**Testing:**

```bash
# Build to verify compilation
cargo build

# Try running (will fail because API doesn't exist yet, but should get to network call)
cargo run -- init --remote --source "postgresql://test" --target "postgresql://test" --yes

# Should see:
# üåê Remote execution mode enabled
# API endpoint: https://api.seren.cloud/replication
# Submitting replication job...
# Error: Failed to submit job (expected - API doesn't exist yet)
```

**Commit:**

```bash
git add src/commands/init.rs
git commit -m "Add --remote flag to init command

- Add --remote and --remote-api CLI flags
- Implement init_remote() for remote execution path
- Build JobSpec from CLI arguments
- Poll for job completion with status updates"
```

---

### Task 2.2: Add environment variable for API endpoint (15 min)

**What:** Allow API endpoint to be configured via environment variable.

**Why:** Makes testing easier, allows different environments (dev/staging/prod).

**Files to modify:**
1. `src/commands/init.rs`

**Steps:**

1. **Update default value to check environment variable**

```rust
#[derive(Parser)]
pub struct InitArgs {
    // ... existing fields ...

    /// API endpoint for remote execution
    #[arg(
        long,
        default_value_t = std::env::var("SEREN_REMOTE_API")
            .unwrap_or_else(|_| "https://api.seren.cloud/replication".to_string())
    )]
    pub remote_api: String,
}
```

**Testing:**

```bash
# Test with custom endpoint
export SEREN_REMOTE_API="http://localhost:3000"
cargo run -- init --remote --source "test" --target "test" --yes

# Should show: API endpoint: http://localhost:3000
```

**Commit:**

```bash
git add src/commands/init.rs
git commit -m "Support SEREN_REMOTE_API environment variable

- Allow custom API endpoint via env var
- Defaults to production API if not set"
```

---

## Hour 3: AWS Lambda Function

### Task 3.1: Create Lambda function structure (30 min)

**What:** Create the AWS Lambda function that receives job submissions and provisions EC2 instances.

**Why:** Lambda is the coordinator that manages job state and launches workers.

**Files to create:**
1. `aws/lambda/handler.py`
2. `aws/lambda/requirements.txt`
3. `aws/lambda/README.md`

**Steps:**

1. **Create aws/lambda/handler.py**

```python
"""
ABOUTME: AWS Lambda function for remote replication job orchestration
ABOUTME: Handles POST /jobs (submit) and GET /jobs/{id} (status) requests
"""

import json
import uuid
import time
import boto3
import os
from datetime import datetime

# AWS clients
dynamodb = boto3.client('dynamodb')
ec2 = boto3.client('ec2')

# Configuration from environment variables
DYNAMODB_TABLE = os.environ.get('DYNAMODB_TABLE', 'replication-jobs')
WORKER_AMI_ID = os.environ.get('WORKER_AMI_ID', 'ami-xxxxxxxxx')
WORKER_INSTANCE_TYPE = os.environ.get('WORKER_INSTANCE_TYPE', 'c5.2xlarge')
WORKER_IAM_ROLE = os.environ.get('WORKER_IAM_ROLE', 'seren-replication-worker')


def lambda_handler(event, context):
    """Main Lambda handler - routes requests to appropriate handler"""

    http_method = event.get('httpMethod', '')
    path = event.get('path', '')

    print(f"Request: {http_method} {path}")

    try:
        if http_method == 'POST' and path == '/jobs':
            return handle_submit_job(event)
        elif http_method == 'GET' and path.startswith('/jobs/'):
            job_id = path.split('/')[-1]
            return handle_get_job(job_id)
        else:
            return {
                'statusCode': 404,
                'body': json.dumps({'error': 'Not found'})
            }
    except Exception as e:
        print(f"Error: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }


def handle_submit_job(event):
    """Handle POST /jobs - submit new replication job"""

    # Parse request body
    try:
        body = json.loads(event['body'])
    except:
        return {
            'statusCode': 400,
            'body': json.dumps({'error': 'Invalid JSON'})
        }

    # Validate required fields
    required_fields = ['command', 'source_url', 'target_url']
    for field in required_fields:
        if field not in body:
            return {
                'statusCode': 400,
                'body': json.dumps({'error': f'Missing required field: {field}'})
            }

    # Generate job ID
    job_id = str(uuid.uuid4())

    # Create job record in DynamoDB
    now = datetime.utcnow().isoformat() + 'Z'
    ttl = int(time.time()) + (30 * 86400)  # 30 days

    dynamodb.put_item(
        TableName=DYNAMODB_TABLE,
        Item={
            'job_id': {'S': job_id},
            'status': {'S': 'provisioning'},
            'command': {'S': body['command']},
            'source_url': {'S': body['source_url']},
            'target_url': {'S': body['target_url']},
            'filter': {'S': json.dumps(body.get('filter', {}))},
            'options': {'S': json.dumps(body.get('options', {}))},
            'created_at': {'S': now},
            'ttl': {'N': str(ttl)},
        }
    )

    # Provision EC2 instance
    try:
        instance_id = provision_worker(job_id, body)

        # Update job with instance ID
        dynamodb.update_item(
            TableName=DYNAMODB_TABLE,
            Key={'job_id': {'S': job_id}},
            UpdateExpression='SET instance_id = :iid',
            ExpressionAttributeValues={':iid': {'S': instance_id}}
        )

        print(f"Job {job_id} submitted, instance {instance_id} provisioning")

    except Exception as e:
        print(f"Failed to provision instance: {e}")
        # Update job status to failed
        dynamodb.update_item(
            TableName=DYNAMODB_TABLE,
            Key={'job_id': {'S': job_id}},
            UpdateExpression='SET #status = :status, error = :error',
            ExpressionAttributeNames={'#status': 'status'},
            ExpressionAttributeValues={
                ':status': {'S': 'failed'},
                ':error': {'S': f'Provisioning failed: {str(e)}'}
            }
        )
        return {
            'statusCode': 500,
            'body': json.dumps({'error': f'Provisioning failed: {str(e)}'})
        }

    return {
        'statusCode': 201,
        'body': json.dumps({
            'job_id': job_id,
            'status': 'provisioning'
        })
    }


def provision_worker(job_id, job_spec):
    """Provision EC2 instance to run replication job"""

    # Build user data script
    user_data = f"""#!/bin/bash
set -euo pipefail

# Write job spec to file
cat > /tmp/job_spec.json <<'EOF'
{json.dumps(job_spec)}
EOF

# Execute worker script
/opt/seren-replicator/worker.sh "{job_id}" /tmp/job_spec.json
"""

    # Launch instance
    response = ec2.run_instances(
        ImageId=WORKER_AMI_ID,
        InstanceType=WORKER_INSTANCE_TYPE,
        MinCount=1,
        MaxCount=1,
        IamInstanceProfile={'Name': WORKER_IAM_ROLE},
        UserData=user_data,
        TagSpecifications=[{
            'ResourceType': 'instance',
            'Tags': [
                {'Key': 'Name', 'Value': f'seren-replication-{job_id}'},
                {'Key': 'JobId', 'Value': job_id},
                {'Key': 'ManagedBy', 'Value': 'seren-replication-system'}
            ]
        }],
        InstanceInitiatedShutdownBehavior='terminate',
    )

    instance_id = response['Instances'][0]['InstanceId']
    return instance_id


def handle_get_job(job_id):
    """Handle GET /jobs/{job_id} - get job status"""

    try:
        response = dynamodb.get_item(
            TableName=DYNAMODB_TABLE,
            Key={'job_id': {'S': job_id}}
        )
    except Exception as e:
        print(f"DynamoDB error: {e}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': 'Database error'})
        }

    if 'Item' not in response:
        return {
            'statusCode': 404,
            'body': json.dumps({'error': 'Job not found'})
        }

    item = response['Item']

    # Convert DynamoDB item to JSON
    job_status = {
        'job_id': item['job_id']['S'],
        'status': item['status']['S'],
        'created_at': item.get('created_at', {}).get('S'),
        'started_at': item.get('started_at', {}).get('S'),
        'completed_at': item.get('completed_at', {}).get('S'),
        'error': item.get('error', {}).get('S'),
    }

    # Parse progress if present
    if 'progress' in item:
        try:
            job_status['progress'] = json.loads(item['progress']['S'])
        except:
            pass

    return {
        'statusCode': 200,
        'body': json.dumps(job_status)
    }
```

2. **Create aws/lambda/requirements.txt**

```
boto3>=1.26.0
```

3. **Create aws/lambda/README.md**

```markdown
# Lambda Function for Remote Replication

This Lambda function orchestrates remote replication jobs.

## Environment Variables

- `DYNAMODB_TABLE`: DynamoDB table name (default: replication-jobs)
- `WORKER_AMI_ID`: AMI ID for worker instances (required)
- `WORKER_INSTANCE_TYPE`: EC2 instance type (default: c5.2xlarge)
- `WORKER_IAM_ROLE`: IAM role name for workers (default: seren-replication-worker)

## Deployment

```bash
# Package Lambda
cd aws/lambda
zip -r lambda.zip handler.py

# Upload to AWS (replace with your function name)
aws lambda update-function-code \
  --function-name seren-replication-coordinator \
  --zip-file fileb://lambda.zip
```

## Testing Locally

```bash
# Install dependencies
pip install -r requirements.txt

# Run tests (TBD)
python -m pytest
```
```

**Testing:**

```bash
# Verify Python syntax
python3 -c "import py_compile; py_compile.compile('aws/lambda/handler.py')"

# Should see no errors
```

**Commit:**

```bash
git add aws/lambda/
git commit -m "Add Lambda function for job orchestration

- Implement POST /jobs for job submission
- Implement GET /jobs/{id} for status retrieval
- Add EC2 instance provisioning logic
- Add DynamoDB integration for job state"
```

---

### Task 3.2: Create DynamoDB table with Terraform (30 min)

**What:** Define DynamoDB table using Terraform (Infrastructure as Code).

**Why:** Terraform makes infrastructure reproducible and version-controlled.

**Files to create:**
1. `aws/terraform/main.tf`
2. `aws/terraform/variables.tf`
3. `aws/terraform/outputs.tf`
4. `aws/terraform/README.md`

**Steps:**

1. **Create aws/terraform/main.tf**

```hcl
# ABOUTME: Terraform configuration for remote replication infrastructure
# ABOUTME: Creates DynamoDB table, Lambda function, and API Gateway

terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
}

# DynamoDB table for job state
resource "aws_dynamodb_table" "replication_jobs" {
  name           = var.dynamodb_table_name
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "job_id"

  attribute {
    name = "job_id"
    type = "S"
  }

  attribute {
    name = "status"
    type = "S"
  }

  attribute {
    name = "created_at"
    type = "S"
  }

  global_secondary_index {
    name            = "status-created_at-index"
    hash_key        = "status"
    range_key       = "created_at"
    projection_type = "ALL"
  }

  ttl {
    attribute_name = "ttl"
    enabled        = true
  }

  tags = {
    Name      = "replication-jobs"
    ManagedBy = "Terraform"
  }
}

# IAM role for Lambda
resource "aws_iam_role" "lambda_role" {
  name = "${var.project_name}-lambda-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "lambda.amazonaws.com"
      }
    }]
  })
}

# IAM policy for Lambda
resource "aws_iam_role_policy" "lambda_policy" {
  name = "${var.project_name}-lambda-policy"
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "dynamodb:PutItem",
          "dynamodb:GetItem",
          "dynamodb:UpdateItem",
        ]
        Resource = aws_dynamodb_table.replication_jobs.arn
      },
      {
        Effect = "Allow"
        Action = [
          "ec2:RunInstances",
          "ec2:CreateTags"
        ]
        Resource = "*"
      },
      {
        Effect = "Allow"
        Action = "iam:PassRole"
        Resource = aws_iam_role.worker_role.arn
      },
      {
        Effect = "Allow"
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents"
        ]
        Resource = "*"
      }
    ]
  })
}

# Lambda function
resource "aws_lambda_function" "coordinator" {
  filename         = "../lambda/lambda.zip"
  function_name    = "${var.project_name}-coordinator"
  role            = aws_iam_role.lambda_role.arn
  handler         = "handler.lambda_handler"
  source_code_hash = filebase64sha256("../lambda/lambda.zip")
  runtime         = "python3.11"
  timeout         = 60

  environment {
    variables = {
      DYNAMODB_TABLE         = aws_dynamodb_table.replication_jobs.name
      WORKER_AMI_ID         = var.worker_ami_id
      WORKER_INSTANCE_TYPE  = var.worker_instance_type
      WORKER_IAM_ROLE       = aws_iam_role.worker_role.name
    }
  }
}

# API Gateway
resource "aws_apigatewayv2_api" "api" {
  name          = "${var.project_name}-api"
  protocol_type = "HTTP"
}

resource "aws_apigatewayv2_integration" "lambda" {
  api_id           = aws_apigatewayv2_api.api.id
  integration_type = "AWS_PROXY"
  integration_uri  = aws_lambda_function.coordinator.invoke_arn
}

resource "aws_apigatewayv2_route" "post_jobs" {
  api_id    = aws_apigatewayv2_api.api.id
  route_key = "POST /jobs"
  target    = "integrations/${aws_apigatewayv2_integration.lambda.id}"
}

resource "aws_apigatewayv2_route" "get_job" {
  api_id    = aws_apigatewayv2_api.api.id
  route_key = "GET /jobs/{job_id}"
  target    = "integrations/${aws_apigatewayv2_integration.lambda.id}"
}

resource "aws_apigatewayv2_stage" "default" {
  api_id      = aws_apigatewayv2_api.api.id
  name        = "$default"
  auto_deploy = true
}

resource "aws_lambda_permission" "api_gateway" {
  statement_id  = "AllowAPIGatewayInvoke"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.coordinator.function_name
  principal     = "apigateway.amazonaws.com"
  source_arn    = "${aws_apigatewayv2_api.api.execution_arn}/*/*"
}

# IAM role for EC2 workers
resource "aws_iam_role" "worker_role" {
  name = "${var.project_name}-worker-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_role_policy" "worker_policy" {
  name = "${var.project_name}-worker-policy"
  role = aws_iam_role.worker_role.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "dynamodb:GetItem",
          "dynamodb:UpdateItem"
        ]
        Resource = aws_dynamodb_table.replication_jobs.arn
      },
      {
        Effect = "Allow"
        Action = [
          "ec2:TerminateInstances"
        ]
        Resource = "*"
        Condition = {
          StringEquals = {
            "ec2:ResourceTag/ManagedBy" = "seren-replication-system"
          }
        }
      }
    ]
  })
}

resource "aws_iam_instance_profile" "worker_profile" {
  name = "${var.project_name}-worker-profile"
  role = aws_iam_role.worker_role.name
}
```

2. **Create aws/terraform/variables.tf**

```hcl
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "us-east-1"
}

variable "project_name" {
  description = "Project name prefix"
  type        = string
  default     = "seren-replication"
}

variable "dynamodb_table_name" {
  description = "DynamoDB table name"
  type        = string
  default     = "replication-jobs"
}

variable "worker_ami_id" {
  description = "AMI ID for worker instances"
  type        = string
  default     = "ami-0c55b159cbfafe1f0" # Placeholder - replace with actual AMI
}

variable "worker_instance_type" {
  description = "EC2 instance type for workers"
  type        = string
  default     = "c5.2xlarge"
}
```

3. **Create aws/terraform/outputs.tf**

```hcl
output "api_endpoint" {
  description = "API Gateway endpoint URL"
  value       = aws_apigatewayv2_api.api.api_endpoint
}

output "dynamodb_table_name" {
  description = "DynamoDB table name"
  value       = aws_dynamodb_table.replication_jobs.name
}

output "lambda_function_name" {
  description = "Lambda function name"
  value       = aws_lambda_function.coordinator.function_name
}

output "worker_iam_role_name" {
  description = "IAM role name for workers"
  value       = aws_iam_role.worker_role.name
}
```

4. **Create aws/terraform/README.md**

```markdown
# Terraform Infrastructure

This directory contains Terraform configuration for remote replication infrastructure.

## Prerequisites

- AWS CLI configured with credentials
- Terraform >= 1.0 installed

## Deployment

```bash
# Initialize Terraform
terraform init

# Review plan
terraform plan

# Apply infrastructure
terraform apply

# Get API endpoint
terraform output api_endpoint
```

## Testing

Export the API endpoint:

```bash
export SEREN_REMOTE_API=$(terraform output -raw api_endpoint)
```

## Cleanup

```bash
terraform destroy
```
```

**Testing:**

```bash
# Install Terraform if not present (macOS)
brew install terraform

# Initialize (will download AWS provider)
cd aws/terraform
terraform init

# Validate configuration
terraform validate

# Should see: Success! The configuration is valid.
```

**Commit:**

```bash
git add aws/terraform/
git commit -m "Add Terraform configuration for AWS infrastructure

- Define DynamoDB table with GSI and TTL
- Define Lambda function and IAM roles
- Define API Gateway with routes
- Define EC2 worker IAM role and instance profile"
```

---

## Hour 4: EC2 Worker Implementation

### Task 4.1: Create EC2 worker bootstrap script (30 min)

**What:** Create the shell script that runs on EC2 to execute replication.

**Why:** This script orchestrates the replication and reports status back to DynamoDB.

**Files to create:**
1. `aws/ec2/worker.sh`
2. `aws/ec2/README.md`

**Steps:**

1. **Create aws/ec2/worker.sh**

```bash
#!/bin/bash
# ABOUTME: Bootstrap script for EC2 worker instances
# ABOUTME: Executes replication job and updates status in DynamoDB

set -euo pipefail

# Arguments
JOB_ID=$1
JOB_SPEC_FILE=$2

# Configuration
DYNAMODB_TABLE=${DYNAMODB_TABLE:-replication-jobs}
AWS_REGION=${AWS_REGION:-us-east-1}
REPLICATOR_BIN="/opt/seren-replicator/postgres-seren-replicator"

# Logging
LOG_FILE="/tmp/replication_${JOB_ID}.log"
exec > >(tee -a "$LOG_FILE")
exec 2>&1

echo "=========================================="
echo "Seren Replication Worker"
echo "Job ID: $JOB_ID"
echo "Started: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
echo "=========================================="

# Update job status to "running"
update_status() {
    local status=$1
    local error_msg=${2:-}

    echo "Updating job status to: $status"

    if [ -z "$error_msg" ]; then
        aws dynamodb update-item \
            --region "$AWS_REGION" \
            --table-name "$DYNAMODB_TABLE" \
            --key "{\"job_id\": {\"S\": \"$JOB_ID\"}}" \
            --update-expression "SET #status = :status, started_at = :started" \
            --expression-attribute-names '{"#status": "status"}' \
            --expression-attribute-values "{\":status\": {\"S\": \"$status\"}, \":started\": {\"S\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"}}"
    else
        aws dynamodb update-item \
            --region "$AWS_REGION" \
            --table-name "$DYNAMODB_TABLE" \
            --key "{\"job_id\": {\"S\": \"$JOB_ID\"}}" \
            --update-expression "SET #status = :status, completed_at = :completed, error = :error" \
            --expression-attribute-names '{"#status": "status"}' \
            --expression-attribute-values "{\":status\": {\"S\": \"$status\"}, \":completed\": {\"S\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"}, \":error\": {\"S\": \"$error_msg\"}}"
    fi
}

# Trap errors
cleanup() {
    local exit_code=$?

    if [ $exit_code -ne 0 ]; then
        echo "ERROR: Script failed with exit code $exit_code"
        local error_msg=$(tail -n 50 "$LOG_FILE" | tr '\n' ' ')
        update_status "failed" "$error_msg"
    fi

    # Terminate self
    INSTANCE_ID=$(ec2-metadata --instance-id | cut -d' ' -f2)
    echo "Terminating instance: $INSTANCE_ID"
    aws ec2 terminate-instances --region "$AWS_REGION" --instance-ids "$INSTANCE_ID" || true
}

trap cleanup EXIT

# Parse job specification
echo "Parsing job specification..."
COMMAND=$(jq -r '.command' "$JOB_SPEC_FILE")
SOURCE_URL=$(jq -r '.source_url' "$JOB_SPEC_FILE")
TARGET_URL=$(jq -r '.target_url' "$JOB_SPEC_FILE")

echo "Command: $COMMAND"
echo "Source: ${SOURCE_URL%%@*}@***" # Mask credentials
echo "Target: ${TARGET_URL%%@*}@***"

# Build replicator command
REPLICATOR_CMD="$REPLICATOR_BIN $COMMAND"
REPLICATOR_CMD="$REPLICATOR_CMD --source '$SOURCE_URL'"
REPLICATOR_CMD="$REPLICATOR_CMD --target '$TARGET_URL'"
REPLICATOR_CMD="$REPLICATOR_CMD --yes"

# Add filter arguments if present
if jq -e '.filter.include_databases' "$JOB_SPEC_FILE" > /dev/null; then
    DATABASES=$(jq -r '.filter.include_databases | join(",")' "$JOB_SPEC_FILE")
    REPLICATOR_CMD="$REPLICATOR_CMD --include-databases '$DATABASES'"
fi

if jq -e '.filter.exclude_tables' "$JOB_SPEC_FILE" > /dev/null; then
    TABLES=$(jq -r '.filter.exclude_tables | join(",")' "$JOB_SPEC_FILE")
    REPLICATOR_CMD="$REPLICATOR_CMD --exclude-tables '$TABLES'"
fi

# Add options
if jq -e '.options.drop_existing' "$JOB_SPEC_FILE" | grep -q true; then
    REPLICATOR_CMD="$REPLICATOR_CMD --drop-existing"
fi

# Update status to running
update_status "running"

# Create workspace
WORKSPACE="/mnt/replication/$JOB_ID"
mkdir -p "$WORKSPACE"
cd "$WORKSPACE"

# Execute replication
echo "=========================================="
echo "Executing: $REPLICATOR_CMD"
echo "=========================================="

if eval "$REPLICATOR_CMD"; then
    echo "=========================================="
    echo "SUCCESS: Replication completed"
    echo "Completed: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
    echo "=========================================="

    aws dynamodb update-item \
        --region "$AWS_REGION" \
        --table-name "$DYNAMODB_TABLE" \
        --key "{\"job_id\": {\"S\": \"$JOB_ID\"}}" \
        --update-expression "SET #status = :status, completed_at = :completed" \
        --expression-attribute-names '{"#status": "status"}' \
        --expression-attribute-values "{\":status\": {\"S\": \"completed\"}, \":completed\": {\"S\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"}}"

    exit 0
else
    echo "=========================================="
    echo "FAILED: Replication failed"
    echo "=========================================="
    exit 1
fi
```

2. **Create aws/ec2/README.md**

```markdown
# EC2 Worker Configuration

This directory contains the bootstrap script for EC2 worker instances.

## Files

- `worker.sh`: Main bootstrap script that executes on instance launch

## Worker AMI Requirements

The worker AMI must include:

1. **PostgreSQL 17 Client Tools**
   ```bash
   sudo apt-get update
   sudo apt-get install -y postgresql-client-17
   ```

2. **postgres-seren-replicator Binary**
   ```bash
   sudo mkdir -p /opt/seren-replicator
   sudo cp target/release/postgres-seren-replicator /opt/seren-replicator/
   sudo chmod +x /opt/seren-replicator/postgres-seren-replicator
   ```

3. **Worker Script**
   ```bash
   sudo cp aws/ec2/worker.sh /opt/seren-replicator/
   sudo chmod +x /opt/seren-replicator/worker.sh
   ```

4. **AWS CLI**
   ```bash
   curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
   unzip awscliv2.zip
   sudo ./aws/install
   ```

5. **jq (JSON parser)**
   ```bash
   sudo apt-get install -y jq
   ```

6. **ec2-metadata tool**
   ```bash
   sudo apt-get install -y cloud-guest-utils
   ```

## Building Worker AMI

See `aws/ec2/build-ami.sh` for automated AMI build script.

## Testing Worker Script Locally

```bash
# Create test job spec
cat > /tmp/test_job.json <<EOF
{
  "command": "init",
  "source_url": "postgresql://user:pass@source:5432/db",
  "target_url": "postgresql://user:pass@target:5432/db",
  "filter": {},
  "options": {"yes": true}
}
EOF

# Run worker script
export DYNAMODB_TABLE=replication-jobs
export AWS_REGION=us-east-1
./aws/ec2/worker.sh test-job-id /tmp/test_job.json
```
```

**Testing:**

```bash
# Verify script syntax
bash -n aws/ec2/worker.sh

# Should see no errors

# Make executable
chmod +x aws/ec2/worker.sh
```

**Commit:**

```bash
git add aws/ec2/
git commit -m "Add EC2 worker bootstrap script

- Parse job specification from JSON
- Build replicator command with filters
- Execute replication and capture output
- Update DynamoDB with status (running, completed, failed)
- Self-terminate on completion"
```

---

### Task 4.2: Create AMI build script (30 min)

**What:** Create a script that builds the worker AMI with all dependencies.

**Why:** Need a reproducible way to create worker AMIs.

**Files to create:**
1. `aws/ec2/build-ami.sh`
2. `aws/ec2/setup-worker.sh`

**Steps:**

1. **Create aws/ec2/setup-worker.sh**

```bash
#!/bin/bash
# ABOUTME: Setup script for worker AMI
# ABOUTME: Installs all dependencies needed for replication workers

set -euxo pipefail

echo "Setting up Seren replication worker..."

# Update system
sudo apt-get update
sudo apt-get upgrade -y

# Install PostgreSQL 17 client tools
sudo apt-get install -y wget gnupg
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo apt-get update
sudo apt-get install -y postgresql-client-17

# Install AWS CLI
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
rm -rf aws awscliv2.zip

# Install jq
sudo apt-get install -y jq

# Install ec2-metadata
sudo apt-get install -y cloud-guest-utils

# Create replicator directory
sudo mkdir -p /opt/seren-replicator

# Note: Binary and worker script will be copied by build script

echo "Worker setup complete!"
```

2. **Create aws/ec2/build-ami.sh**

```bash
#!/bin/bash
# ABOUTME: Build worker AMI with Packer
# ABOUTME: Creates AMI with all dependencies and replicator binary

set -euo pipefail

echo "Building worker AMI..."

# Check prerequisites
if ! command -v packer &> /dev/null; then
    echo "ERROR: Packer not installed. Install from: https://www.packer.io/"
    exit 1
fi

if [ ! -f "target/release/postgres-seren-replicator" ]; then
    echo "ERROR: Binary not found. Run: cargo build --release"
    exit 1
fi

# Create temporary directory for AMI build
BUILD_DIR=$(mktemp -d)
echo "Build directory: $BUILD_DIR"

# Copy files
cp target/release/postgres-seren-replicator "$BUILD_DIR/"
cp aws/ec2/worker.sh "$BUILD_DIR/"
cp aws/ec2/setup-worker.sh "$BUILD_DIR/"

# Create Packer template
cat > "$BUILD_DIR/worker-ami.pkr.hcl" <<'EOF'
packer {
  required_plugins {
    amazon = {
      version = ">= 1.0.0"
      source  = "github.com/hashicorp/amazon"
    }
  }
}

variable "aws_region" {
  type    = string
  default = "us-east-1"
}

source "amazon-ebs" "ubuntu" {
  ami_name      = "seren-replication-worker-{{timestamp}}"
  instance_type = "t3.medium"
  region        = var.aws_region
  source_ami_filter {
    filters = {
      name                = "ubuntu/images/*ubuntu-jammy-22.04-amd64-server-*"
      root-device-type    = "ebs"
      virtualization-type = "hvm"
    }
    most_recent = true
    owners      = ["099720109477"]
  }
  ssh_username = "ubuntu"

  tags = {
    Name      = "seren-replication-worker"
    ManagedBy = "Packer"
  }
}

build {
  sources = ["source.amazon-ebs.ubuntu"]

  # Upload files
  provisioner "file" {
    source      = "setup-worker.sh"
    destination = "/tmp/setup-worker.sh"
  }

  provisioner "file" {
    source      = "postgres-seren-replicator"
    destination = "/tmp/postgres-seren-replicator"
  }

  provisioner "file" {
    source      = "worker.sh"
    destination = "/tmp/worker.sh"
  }

  # Run setup
  provisioner "shell" {
    inline = [
      "chmod +x /tmp/setup-worker.sh",
      "/tmp/setup-worker.sh",
      "sudo mkdir -p /opt/seren-replicator",
      "sudo mv /tmp/postgres-seren-replicator /opt/seren-replicator/",
      "sudo mv /tmp/worker.sh /opt/seren-replicator/",
      "sudo chmod +x /opt/seren-replicator/postgres-seren-replicator",
      "sudo chmod +x /opt/seren-replicator/worker.sh",
      "sudo mkdir -p /mnt/replication",
      "sudo chown ubuntu:ubuntu /mnt/replication"
    ]
  }
}
EOF

# Build AMI
cd "$BUILD_DIR"
packer init worker-ami.pkr.hcl
packer build worker-ami.pkr.hcl

# Extract AMI ID from output
AMI_ID=$(packer build worker-ami.pkr.hcl 2>&1 | grep -oP 'ami-\w+' | tail -1)

echo "=========================================="
echo "AMI built successfully!"
echo "AMI ID: $AMI_ID"
echo "=========================================="
echo ""
echo "Update Terraform variable:"
echo "  worker_ami_id = \"$AMI_ID\""
echo ""

# Cleanup
rm -rf "$BUILD_DIR"
```

**Testing:**

```bash
# Verify script syntax
bash -n aws/ec2/build-ami.sh
bash -n aws/ec2/setup-worker.sh

# Make executable
chmod +x aws/ec2/build-ami.sh
chmod +x aws/ec2/setup-worker.sh
```

**Commit:**

```bash
git add aws/ec2/build-ami.sh aws/ec2/setup-worker.sh
git commit -m "Add AMI build automation

- Add setup-worker.sh for dependency installation
- Add build-ami.sh for Packer-based AMI creation
- Includes PostgreSQL 17, AWS CLI, jq, replicator binary"
```

---

## Hour 5-6: Integration and Testing

### Task 5.1: Deploy infrastructure to AWS (30 min)

**What:** Actually deploy the infrastructure to AWS.

**Why:** Need to test end-to-end with real infrastructure.

**Steps:**

1. **Build the replicator binary**

```bash
cargo build --release
```

2. **Build the worker AMI**

```bash
# Install Packer if needed
brew install packer

# Build AMI (takes ~10 minutes)
./aws/ec2/build-ami.sh

# Save the AMI ID from output
export WORKER_AMI_ID="ami-xxxxxxxxx"
```

3. **Package Lambda function**

```bash
cd aws/lambda
zip lambda.zip handler.py
cd ../..
```

4. **Deploy with Terraform**

```bash
cd aws/terraform

# Initialize
terraform init

# Apply with AMI ID
terraform apply -var="worker_ami_id=$WORKER_AMI_ID"

# Save API endpoint
export SEREN_REMOTE_API=$(terraform output -raw api_endpoint)
echo "API Endpoint: $SEREN_REMOTE_API"

cd ../..
```

**Testing:**

```bash
# Test API endpoint is reachable
curl -X POST "$SEREN_REMOTE_API/jobs" \
  -H "Content-Type: application/json" \
  -d '{"command":"init","source_url":"test","target_url":"test"}'

# Should get 201 Created with job_id
```

**Commit:**

```bash
git add .
git commit -m "Deploy infrastructure to AWS

- Built release binary
- Created worker AMI: $WORKER_AMI_ID
- Deployed Lambda and API Gateway
- API endpoint: $SEREN_REMOTE_API"
```

---

### Task 5.2: End-to-end integration test (45 min)

**What:** Test the entire flow with real databases.

**Why:** Verify everything works together.

**Steps:**

1. **Set up test databases**

```bash
# Start test databases with Docker
docker run -d --name test-source \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=testdb \
  -p 5432:5432 \
  postgres:17

docker run -d --name test-target \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=testdb \
  -p 5433:5432 \
  postgres:17

# Wait for databases to start
sleep 10

# Create test data
psql "postgresql://postgres:postgres@localhost:5432/testdb" <<EOF
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    email TEXT NOT NULL
);

INSERT INTO users (name, email) VALUES
    ('Alice', 'alice@example.com'),
    ('Bob', 'bob@example.com'),
    ('Charlie', 'charlie@example.com');
EOF
```

2. **Run remote replication**

```bash
# Export API endpoint
export SEREN_REMOTE_API=$(cd aws/terraform && terraform output -raw api_endpoint)

# Run replication
cargo run --release -- init --remote \
  --source "postgresql://postgres:postgres@host.docker.internal:5432/testdb" \
  --target "postgresql://postgres:postgres@host.docker.internal:5433/testdb" \
  --yes

# Should see:
# üåê Remote execution mode enabled
# Submitting replication job...
# ‚úì Job submitted
# Job ID: <uuid>
# Polling for status...
# Status: provisioning EC2 instance...
# Status: running...
# ‚úì Replication completed successfully
```

3. **Verify data replicated**

```bash
# Check target database
psql "postgresql://postgres:postgres@localhost:5433/testdb" -c "SELECT * FROM users;"

# Should see:
#  id |  name   |       email
# ----+---------+-------------------
#   1 | Alice   | alice@example.com
#   2 | Bob     | bob@example.com
#   3 | Charlie | charlie@example.com
```

4. **Test failure case**

```bash
# Test with invalid source
cargo run --release -- init --remote \
  --source "postgresql://invalid:invalid@nonexistent:5432/db" \
  --target "postgresql://postgres:postgres@localhost:5433/testdb" \
  --yes

# Should see:
# üåê Remote execution mode enabled
# Submitting replication job...
# ‚úì Job submitted
# Status: provisioning...
# Status: running...
# ‚úó Replication failed: Failed to connect to source database
```

5. **Clean up test databases**

```bash
docker stop test-source test-target
docker rm test-source test-target
```

**Commit:**

```bash
git add .
git commit -m "Successful end-to-end integration test

Tested:
- Job submission to API Gateway
- EC2 worker provisioning
- Replication execution
- Status updates in DynamoDB
- Data verification
- Error handling"
```

---

## Hour 7: Documentation

### Task 7.1: Update README with remote execution docs (30 min)

**What:** Document the remote execution feature in the main README.

**Why:** Users need to know how to use remote execution.

**Files to modify:**
1. `README.md`

**Steps:**

1. **Add Remote Execution section to README**

Find the "Usage" section and add:

````markdown
### Remote Execution (AWS)

For long-running replications, use remote execution on AWS infrastructure to avoid network reliability issues with laptop WiFi.

**Prerequisites:**
- AWS account with deployed infrastructure (see [AWS Setup](docs/aws-setup.md))
- API endpoint configured

**Usage:**

```bash
# Set API endpoint (one-time setup)
export SEREN_REMOTE_API="https://your-api-gateway-url.amazonaws.com"

# Run replication remotely
postgres-seren-replicator init --remote \
  --source "postgresql://..." \
  --target "postgresql://..."

# The CLI will:
# 1. Submit job to AWS
# 2. Poll for status every 5 seconds
# 3. Display progress updates
# 4. Exit when complete or failed
```

**Benefits:**
- ‚úÖ No network interruptions (runs on AWS infrastructure)
- ‚úÖ No laptop sleep issues (job continues if you close laptop)
- ‚úÖ Automatic retry and recovery
- ‚úÖ Job history and logs in DynamoDB

**How it works:**
1. CLI submits job specification to API Gateway
2. Lambda function provisions EC2 worker instance
3. Worker executes replication using same binary
4. Worker updates status in DynamoDB
5. Worker terminates when complete
6. CLI polls for status and displays results
````

2. **Create docs/aws-setup.md**

```markdown
# AWS Infrastructure Setup

This guide covers deploying the remote execution infrastructure to AWS.

## Prerequisites

- AWS account with appropriate permissions
- AWS CLI configured (`aws configure`)
- Terraform >= 1.0 installed
- Packer installed (for AMI builds)

## Architecture

```
User CLI ‚Üí API Gateway ‚Üí Lambda ‚Üí DynamoDB
                           ‚Üì
                        EC2 Worker
```

## Deployment Steps

### 1. Build Replicator Binary

```bash
cargo build --release
```

### 2. Build Worker AMI

```bash
./aws/ec2/build-ami.sh
```

Save the AMI ID from the output.

### 3. Deploy Infrastructure

```bash
cd aws/terraform

# Initialize Terraform
terraform init

# Deploy (replace with your AMI ID)
terraform apply -var="worker_ami_id=ami-xxxxxxxxx"

# Save API endpoint
terraform output api_endpoint
```

### 4. Configure CLI

```bash
# Add to ~/.bashrc or ~/.zshrc
export SEREN_REMOTE_API=$(cd aws/terraform && terraform output -raw api_endpoint)
```

### 5. Test

```bash
postgres-seren-replicator init --remote \
  --source "postgresql://..." \
  --target "postgresql://..." \
  --yes
```

## Cost Estimates

- **Per replication (100GB):** ~$10
  - EC2: $0.68 (2 hours √ó $0.34/hour)
  - Data transfer: $9.00 (100GB √ó $0.09/GB)
  - Lambda/DynamoDB: ~$0.03

- **Monthly infrastructure:** ~$5
  - DynamoDB on-demand: $5/month
  - Lambda: Free tier

## Security

- Source credentials encrypted in transit (TLS)
- Credentials stored in DynamoDB (encrypted at rest)
- EC2 workers run in private subnets
- IAM roles with least privilege

## Monitoring

- CloudWatch Logs: All worker output
- DynamoDB: Job history for 30 days
- EC2: Instance metrics

## Troubleshooting

### Job stuck in "provisioning"

Check Lambda logs:
```bash
aws logs tail /aws/lambda/seren-replication-coordinator --follow
```

### Job failed with "Provisioning failed"

Check EC2 console for failed instance launches. Common issues:
- AMI not found in region
- IAM role doesn't exist
- Insufficient capacity

### Worker logs

```bash
# Find instance ID from job
aws dynamodb get-item \
  --table-name replication-jobs \
  --key '{"job_id": {"S": "your-job-id"}}'

# Check instance status
aws ec2 describe-instances --instance-ids i-xxxxxxxxx
```

## Cleanup

```bash
cd aws/terraform
terraform destroy
```
```

**Commit:**

```bash
git add README.md docs/aws-setup.md
git commit -m "Add remote execution documentation

- Add Remote Execution section to README
- Create AWS setup guide
- Document architecture and costs
- Add troubleshooting guide"
```

---

### Task 7.2: Add security notes to CLAUDE.md (15 min)

**What:** Document security considerations for the remote execution feature.

**Why:** Future developers need to understand security model and risks.

**Files to modify:**
1. `CLAUDE.md`

**Steps:**

Add this section after "Code Security":

```markdown
### Remote Execution Security

**Current Status (MVP):**
- ‚ö†Ô∏è No authentication on API Gateway (anyone can submit jobs)
- ‚ö†Ô∏è Source credentials passed in plaintext to Lambda
- ‚ö†Ô∏è Credentials stored unencrypted in DynamoDB
- ‚ö†Ô∏è No rate limiting or cost controls

**These are acceptable for MVP/internal use only.** Before public release, implement:

1. **Authentication**
   - Add API key authentication to API Gateway
   - Or use AWS IAM authentication
   - Implement per-user job limits

2. **Credential Encryption**
   - Encrypt source credentials with AWS KMS before storing
   - EC2 workers decrypt with IAM role permissions
   - Never log plaintext credentials

3. **Network Security**
   - Deploy Lambda in VPC with private subnets
   - EC2 workers in private subnets (no public IP)
   - VPC endpoints for AWS services
   - NAT Gateway for outbound database access

4. **Audit and Monitoring**
   - CloudTrail for API Gateway access logs
   - Alerts for unusual activity (high job volume, failures)
   - Automatic cleanup of old jobs

5. **Rate Limiting**
   - API Gateway request throttling
   - Per-user concurrent job limits
   - Budget alerts for AWS costs

**Security Checklist for Public Release:**
- [ ] Implement API authentication
- [ ] Add KMS encryption for credentials
- [ ] Deploy in VPC with private subnets
- [ ] Enable CloudTrail logging
- [ ] Add rate limiting
- [ ] Security audit by external party
- [ ] Penetration testing
```

**Commit:**

```bash
git add CLAUDE.md
git commit -m "Document remote execution security considerations

- List current MVP security gaps
- Document required improvements for public release
- Add security checklist"
```

---

## Hour 8: Polish and Cleanup

### Task 8.1: Add integration tests (30 min)

**What:** Add automated integration tests for remote execution.

**Why:** Ensure remote path doesn't break with future changes.

**Files to create:**
1. `tests/integration_remote_test.rs`

**Steps:**

1. **Create tests/integration_remote_test.rs**

```rust
// ABOUTME: Integration tests for remote execution
// ABOUTME: These tests require AWS infrastructure to be deployed

use std::env;

#[tokio::test]
#[ignore] // Run with: cargo test --test integration_remote_test -- --ignored
async fn test_remote_job_submission() {
    let api_endpoint = env::var("SEREN_REMOTE_API")
        .expect("SEREN_REMOTE_API environment variable not set");

    let client = postgres_seren_replicator::remote::RemoteClient::new(api_endpoint)
        .expect("Failed to create client");

    let job_spec = postgres_seren_replicator::remote::JobSpec {
        version: "1".to_string(),
        command: "init".to_string(),
        source_url: "postgresql://test:test@localhost:5432/test".to_string(),
        target_url: "postgresql://test:test@localhost:5433/test".to_string(),
        filter: None,
        options: std::collections::HashMap::new(),
    };

    let response = client
        .submit_job(&job_spec)
        .await
        .expect("Failed to submit job");

    assert!(!response.job_id.is_empty());
    assert_eq!(response.status, "provisioning");

    // Get job status
    let status = client
        .get_job_status(&response.job_id)
        .await
        .expect("Failed to get job status");

    assert_eq!(status.job_id, response.job_id);
    println!("Job submitted: {}", status.job_id);
}

#[tokio::test]
#[ignore]
async fn test_remote_job_polling() {
    let api_endpoint = env::var("SEREN_REMOTE_API")
        .expect("SEREN_REMOTE_API environment variable not set");

    let client = postgres_seren_replicator::remote::RemoteClient::new(api_endpoint)
        .expect("Failed to create client");

    let job_spec = postgres_seren_replicator::remote::JobSpec {
        version: "1".to_string(),
        command: "init".to_string(),
        source_url: "postgresql://invalid:invalid@nonexistent:5432/db".to_string(),
        target_url: "postgresql://test:test@localhost:5433/test".to_string(),
        filter: None,
        options: std::collections::HashMap::new(),
    };

    let response = client
        .submit_job(&job_spec)
        .await
        .expect("Failed to submit job");

    // Poll for completion (should fail)
    let mut poll_count = 0;
    let final_status = client
        .poll_until_complete(&response.job_id, |status| {
            poll_count += 1;
            println!("Status: {:?}", status.status);
        })
        .await
        .expect("Failed to poll");

    assert!(poll_count > 0, "Should have polled at least once");
    assert_eq!(final_status.status, "failed");
    assert!(final_status.error.is_some());
    println!("Final status: {:?}", final_status);
}
```

**Testing:**

```bash
# Run integration tests (requires deployed infrastructure)
export SEREN_REMOTE_API=$(cd aws/terraform && terraform output -raw api_endpoint)
cargo test --test integration_remote_test -- --ignored

# Should see 2 tests pass
```

**Commit:**

```bash
git add tests/integration_remote_test.rs
git commit -m "Add integration tests for remote execution

- Test job submission
- Test status polling
- Test failure handling
- Requires SEREN_REMOTE_API environment variable"
```

---

### Task 8.2: Update CHANGELOG and version (15 min)

**What:** Document the new feature in CHANGELOG.

**Why:** Track changes for users and releases.

**Files to modify:**
1. `CHANGELOG.md` (create if doesn't exist)
2. `Cargo.toml`

**Steps:**

1. **Create/update CHANGELOG.md**

```markdown
# Changelog

All notable changes to this project will be documented in this file.

## [Unreleased]

### Added
- **Remote execution mode**: Run replication on AWS infrastructure to avoid network reliability issues
  - New `--remote` flag for `init` command
  - API Gateway + Lambda + DynamoDB + EC2 architecture
  - Automatic job submission, polling, and status updates
  - Self-terminating EC2 workers
  - Job history stored for 30 days
- AWS infrastructure automation with Terraform
- Worker AMI build automation with Packer
- Integration tests for remote execution
- Documentation for AWS setup and deployment

### Changed
- None

### Fixed
- None

## [2.4.2] - 2025-01-XX

(Previous release notes...)
```

2. **Update Cargo.toml version**

```toml
[package]
name = "postgres-seren-replicator"
version = "2.5.0"  # Bump minor version for new feature
```

**Commit:**

```bash
git add CHANGELOG.md Cargo.toml
git commit -m "Bump version to 2.5.0 and update CHANGELOG

- Document remote execution feature
- Bump minor version (breaking: new dependencies)"
```

---

### Task 8.3: Final verification (15 min)

**What:** Run all tests and verify everything works.

**Why:** Ensure no regressions before shipping.

**Steps:**

```bash
# 1. Format code
cargo fmt

# 2. Run clippy
cargo clippy --all-targets --all-features -- -D warnings

# 3. Run unit tests
cargo test

# 4. Run integration tests (requires test databases)
export TEST_SOURCE_URL="postgresql://postgres:postgres@localhost:5432/postgres"
export TEST_TARGET_URL="postgresql://postgres:postgres@localhost:5433/postgres"
cargo test --test integration_test -- --ignored

# 5. Build release binary
cargo build --release

# 6. Run remote integration test (requires AWS)
export SEREN_REMOTE_API=$(cd aws/terraform && terraform output -raw api_endpoint)
cargo test --test integration_remote_test -- --ignored

# 7. Test CLI help
./target/release/postgres-seren-replicator init --help | grep remote

# Should see:
#   --remote     Execute replication remotely on AWS infrastructure
```

**Commit:**

```bash
git add .
git commit -m "Final verification - all tests passing

- cargo fmt
- cargo clippy (no warnings)
- Unit tests: passing
- Integration tests: passing
- Remote tests: passing
- CLI help updated"
```

---

## Post-Implementation Tasks

### Create GitHub Issues for Future Work

Now create issues for security hardening (post-MVP):

```bash
# Issue 1: Add API authentication
gh issue create --title "Add API authentication to remote execution" --body "$(cat <<'EOF'
## Problem
Currently API Gateway has no authentication - anyone can submit jobs.

## Solution
Add API key authentication to API Gateway:
- Generate API keys for users
- Add x-api-key header requirement
- Implement usage plans and throttling

## Files to Modify
- aws/terraform/main.tf (add API key resources)
- src/remote/client.rs (add API key header)
- README.md (document API key setup)

## Testing
- Verify requests without key are rejected
- Verify requests with invalid key are rejected
- Verify requests with valid key succeed
EOF
)"

# Issue 2: Add KMS encryption
gh issue create --title "Add KMS encryption for source credentials" --body "$(cat <<'EOF'
## Problem
Source credentials stored in plaintext in DynamoDB.

## Solution
Encrypt credentials with AWS KMS:
- Lambda encrypts before storing in DynamoDB
- Worker decrypts with IAM role
- Never log plaintext credentials

## Files to Modify
- aws/lambda/handler.py (add KMS encrypt/decrypt)
- aws/terraform/main.tf (add KMS key and policies)
- aws/ec2/worker.sh (decrypt credentials)

## Testing
- Verify credentials encrypted in DynamoDB
- Verify worker can decrypt
- Verify encryption/decryption errors handled
EOF
)"

# Issue 3: Add VPC deployment
gh issue create --title "Deploy Lambda and EC2 in VPC" --body "$(cat <<'EOF'
## Problem
Lambda and EC2 run in default VPC with public access.

## Solution
Create VPC with private subnets:
- Lambda in private subnet
- EC2 workers in private subnet (no public IP)
- VPC endpoints for AWS services
- NAT Gateway for outbound access

## Files to Modify
- aws/terraform/main.tf (add VPC resources)
- aws/terraform/variables.tf (add VPC config)

## Testing
- Verify Lambda can access DynamoDB via VPC endpoint
- Verify EC2 can access databases via NAT Gateway
- Verify no public IPs assigned
EOF
)"
```

---

## Summary

You've successfully implemented remote execution in 1 day! Here's what was built:

### Rust CLI Enhancements
- ‚úÖ `src/remote/` module with models and client
- ‚úÖ `--remote` flag for init command
- ‚úÖ Job submission and polling logic
- ‚úÖ Progress display and error handling

### AWS Infrastructure
- ‚úÖ Lambda function for job orchestration
- ‚úÖ DynamoDB table for job state
- ‚úÖ API Gateway with POST/GET endpoints
- ‚úÖ IAM roles for Lambda and EC2
- ‚úÖ Terraform automation

### EC2 Worker
- ‚úÖ Bootstrap script for job execution
- ‚úÖ AMI build automation
- ‚úÖ Status updates to DynamoDB
- ‚úÖ Self-termination on completion

### Testing & Docs
- ‚úÖ Integration tests
- ‚úÖ End-to-end test with real databases
- ‚úÖ README documentation
- ‚úÖ AWS setup guide
- ‚úÖ Security considerations

### What's Next (Post-MVP)
- üî≤ API authentication
- üî≤ KMS credential encryption
- üî≤ VPC deployment
- üî≤ Rate limiting
- üî≤ Cost monitoring
- üî≤ CloudWatch dashboards

**Total Time: 8 hours** ‚è∞

**Lines of Code:**
- Rust: ~400 lines
- Python: ~250 lines
- Terraform: ~200 lines
- Bash: ~200 lines
- Tests: ~100 lines
- **Total: ~1,150 lines**

**Commits: 15** (one per task, following TDD)
